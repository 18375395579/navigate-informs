<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1" /><br>Title: Large-scale Distributed Optimization in Deep Learning</br><br>Author: Rajat Monga, Google, 1600 Amphitheatre Parkway, Mountain View, CA, United States of America, rajatmonga@google.com</br><br>Year: 2013</br><br>Abstract: Biologically inspired deep learning models have been achieving state of the art results in areas such as vision and speech and language. We have trained large models (with over 1 billion parameters) utilizing clusters of thousands of machines by leveraging model and data parallelism. In this talk I’ll describe two optimization methods that have been key to achieving these results (i) asynchronous stochastic gradient descent, and (ii) a large batch optimization procedure using L-BFGS.</br>