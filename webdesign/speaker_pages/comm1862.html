<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1" /><br>Title: New Sampling Schemes for Simulation-based Approximate Dynamic Programming</br><br>Author: Dimitri Bertsekas, Massachusetts Institute of Technology, Laboratory for Information and Decision, 77 Massachusetts Avenue, 32-D660, Cambridge, MA, United States of America, dimitrib@mit.edu</br><br>Coauthor(s): Huizhen Yu</br><br>Year: 2012</br><br>Abstract: Existing multistep methods in temporal difference (TD) learning often use a single long trajectory. We propose much more flexible sampling schemes. They are connected with weighted Bellman equations, which broadly generalize the TD(lambda) approximation framework. We provide a convergence analysis, and we review a variety of useful schemes in the context of policy evaluation and policy iteration with exploration enhancements.</br>