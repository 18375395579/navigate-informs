<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1" />
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" >
<br>Title: Approximate Dynamic Programming for Bayesian Partially Observable Reinforcement Learning</br><br>Author: Pascal Poupart, University of Waterloo, 200 University Avenue West, Waterloo, ON, Canada, ppoupart@cs.uwaterloo.ca</br><br>Coauthor(s): Nikos Vlassis</br><br>Year: 2008</br><br>Abstract: The analytical form of the optimal value function for discrete model-based Bayesian reinforcement learning problems is known to be a set of linear combinations of products of Dirichlets.  However, exact dynamic programming is rarely possible since the size of this set and the number of terms in each linear combination can grow exponentially with the planning horizon.  We present various approximation techniques to keep the size of the set and each linear combination manageable.</br>