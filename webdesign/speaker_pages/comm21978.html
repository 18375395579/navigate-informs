<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1" /><br>Title: Q-Learning Algorithms and Their Convergence in Total Cost Problems of Stochastic Shortest Path Type</br><br>Author: Huizhen Yu, Massachusetts Institute of Technology, Laboratory for Information and Decision, 77 Massachusetts Avenue, 32-D558, Cambridge, MA, United States of America, janey_yu@mit.edu</br><br>Coauthor(s): Dimitri Bertsekas</br><br>Year: 2012</br><br>Abstract: Q-learning is an asynchronous stochastic iterative algorithm for solving Markov decision processes (MDP) or two-player zero-sum stochastic games. Its convergence in the total cost case was known earlier under restrictive conditions that ensure boundedness of iterates. We show that such conditions can be removed for MDP and stochastic games of the stochastic shortest path (SSP) type. This convergence result applies also to a new policy iteration-like Q-learning algorithm for SSP problems.</br>