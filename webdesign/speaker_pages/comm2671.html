Title: (Exponentiated) Stochastic Gradient Descent for L1 Constrained Problems<br>Author: Sham Kakade, Assistant Professor, Toyota Technological Institute, 1427 E. 60th Street, Chicago, IL, 60637, United States, sham@tti-c.org<br>Coauthor(s): Eyal Even-Dar, Dean Foster<br>Year: 2006<br>Abstract: Convex optimization problems with L1 constraints often underly such tasks as feature selection and obtaining sparse representations. We show the exponentiated gradient algorithm is effective as a stochastic gradient descent algorithm under general convex loss functions when there are many irrelevant dimensions - requiring a number of gradient steps only logarithmic in the number of dimensions. This includes supervised learning problems under the square, hinge, logistic, and absolute loss.