<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1" />
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" >
<br>Title: (Exponentiated) Stochastic Gradient Descent for L1 Constrained Problems</br><br>Author: Sham Kakade, Assistant Professor, Toyota Technological Institute, 1427 E. 60th Street, Chicago, IL, 60637, United States, sham@tti-c.org</br><br>Coauthor(s): Eyal Even-Dar, Dean Foster</br><br>Year: 2006</br><br>Abstract: Convex optimization problems with L1 constraints often underly such tasks as feature selection and obtaining sparse representations. We show the exponentiated gradient algorithm is effective as a stochastic gradient descent algorithm under general convex loss functions when there are many irrelevant dimensions - requiring a number of gradient steps only logarithmic in the number of dimensions. This includes supervised learning problems under the square, hinge, logistic, and absolute loss.</br>