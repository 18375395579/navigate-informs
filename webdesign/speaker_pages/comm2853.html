<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1" />
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" >
<br>Title: MIPLIB 20XX: On the Difficulties to Find a Good Set of MIP Instances for Benchmarking</br><br>Author: Thorsten Koch, Senior Scientist, Konrad-Zuse-Zentrum für Informationstechnik Berlin, Takustraße 7, 14195 Berlin-Dahlem, Germany, koch@zib.de</br><br>Coauthor(s): Alexander Martin, Tobias Achterberg</br><br>Year: 2006</br><br>Abstract: Benchmarking MIP solvers is difficult at best.  How could we compare the performance of two solvers in a fair and meaningful way?  What are suitable criteria for selecting problem instances in a benchmark? Only difficult to solve instances?  Constructing unsolvable instances is always possible. Only real-world examples? People who want results tend to use models that work well with existing solver technology. We will try to propose some answers to these questions. Discussion welcome.</br>