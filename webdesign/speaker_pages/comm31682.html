<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1" /><br>Title: Bayesian Multi-armed Bandits in Recommender Systems</br><br>Author: Xiaoting Zhao, Cornell University, Rhodes Hall, Ithaca, NY, 14850, United States of America, xz337@cornell.edu</br><br>Coauthor(s): Peter Frazier</br><br>Year: 2012</br><br>Abstract: We consider a Bayesian contextual multi-armed bandit problem arising in recommender systems, motivated by an application to the arXiV repository of scholarly papers.  In this problem, we wish to recommend items (e.g., movies, or scientific articles) to users of a website, and use the feedback to better learn their preferences.  We provide an algorithm that is Bayes-optimal under a simplifying orthogonality assumption, as well as a general upper bound on the value of the Bayes-optimal algorithm.</br>