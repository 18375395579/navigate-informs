Title: Improving Repeated Labeling for Crowdsourced Data Annotation<br>Author: Sergiu Goschin, Computer Science Department, Rutgers University, Piscataway, NY, 08854, United States of America, sgoschin@gmail.com<br>Coauthor(s): Haym Hirsh<br>Year: 2013<br>Abstract: Crowdsourcing resources such as Amazon Mechanical Turk make it possible to label data cheaply, but often unreliability. A common approach for improving accuracy is to get labels from multiple workers, using their majority vote as the final label. We present a new approach, Beat-by-k, which obtains labels incrementally until one label is seen k more times than the other.  Our results show that Beat-by-k requires fewer labels to reach a given level of accuracy compared to other approaches.