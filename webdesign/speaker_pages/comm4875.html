<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1" /><br>Title: Learning to Optimize Via Posterior Sampling</br><br>Author: Dan Russo, Stanford University, 215 Ayrshire Farm Lane, Stanford, CA, United States of America, dan.joseph.russo@gmail.com</br><br>Coauthor(s): Benjamin Van Roy</br><br>Year: 2013</br><br>Abstract: We consider the application of a simple posterior sampling algorithm to multi-armed bandit problems. The algorithm, also known as Thompson Sampling, can be applied to problems with large action spaces and complicated dependencies among action rewards. We establish a theoretical connection between posterior sampling and the popular upper confidence bound approach. This relationship yields insight into the algorithm’s performance advantages, and leads to a number of new theoretical guarantees.</br>