Title: Learning to Optimize Via Posterior Sampling<br>Author: Dan Russo, Stanford University, 215 Ayrshire Farm Lane, Stanford, CA, United States of America, dan.joseph.russo@gmail.com<br>Coauthor(s): Benjamin Van Roy<br>Year: 2013<br>Abstract: We consider the application of a simple posterior sampling algorithm to multi-armed bandit problems. The algorithm, also known as Thompson Sampling, can be applied to problems with large action spaces and complicated dependencies among action rewards. We establish a theoretical connection between posterior sampling and the popular upper confidence bound approach. This relationship yields insight into the algorithm’s performance advantages, and leads to a number of new theoretical guarantees.