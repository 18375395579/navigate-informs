<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1" />
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" >
<br>Title: The Khachiyan Prize for Lifetime Accomplishments in Optimization - Donald Goldfarb and Alexander Shapiro</br><br>Author: Jorge Nocedal, Professor, Northwestern University, Evanston, IL, United States of America, nocedal@eecs.northwestern.edu</br><br>Year: 2013</br><br>Abstract: Donald Goldfarb has made fundamental contributions to the field of continuous optimization through the design and analysis of innovative algorithms, including the celebrated BFGS quasi-Newton method for nonlinear optimization and the steepest edge simplex method for linear programming.    Alexander Shapiro has been one of the most prolific scholars in the field of Operations Research, contributing significantly to nonlinear analysis (specifically sensitivity and optimality), and to stochastic programming, where his work on complexity analysis and risk-averse decision making has been highly influential.      Community Member Log-in:              Forgot your username or password        Participate in Communities                      You are here:   Home   Community   Optimization Society   Optimization Society Prizes   Student-Paper-Prize   2013                Afonso Bandeira is selected as the winner of the 2013 INFORMS Optimization Society Student Paper Prize            Citation    Afonso Bandeira (Princeton University) is selected as the winner of the 2013 INFORMS Optimization Society Student Paper Prize for his paper Computation of Sparse Low Degree Interpolating Polynomials and their Application to Derivative-Free Optimization,?with Katya Scheinberg and Luis N. Vicente.    The novelty of this paper is in the application of sparse signal recovery to the construction of polynomial interpolation models of functions with sparse Hessians. In general, constructing an accurate second-order interpolation model of a smooth function requires O(n^2) samples. Bandeira et al. show that if the Hessian contains only s nonzeros, then using sparse recovery via l-1 minimization, one can construct an accurate second-order model using only O(s log^4(n)) samples. When applied to derivative-free optimization, the techniques developed significantly reduce the number of evaluations necessary for function approximation via sampling, and thus accelerate the optimization of complex systems such as those whose objective functions are calculated by a costly black-box simulation.</br>