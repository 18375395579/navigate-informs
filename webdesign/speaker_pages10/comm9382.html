<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1" />
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" >
        <br>Title: Feedback and Incentives in Crowdsourcing Contests: An Empirical Analysis</br><br>Author: Jiahui Mo, University of Texas at Dallas, 800 W Campbell Road, SM 33, Richardson, TX, 75080, United States of America, jiahui.mo@utdallas.edu</br><br>Coauthor(s): Xianjun Geng, Zhiqiang Zheng</br><br>Year: 2012</br><br>Abstract: In a crowdsourcing contest, a seeker often provides performance feedbacks to solvers. This paper empirically investigates the relationship between seeker feedback and solver behavior. We find that feedback does not necessarily increase the overall efforts of participating solvers. Different types of feedback exhibit different effects in attracting new solvers. The effect of feedbacks is found to be contingent on the type of solvers.</br>